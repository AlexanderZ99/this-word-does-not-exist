{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urban_dictionary_scraper\n",
    "import torch\n",
    "import re\n",
    "import sys\n",
    "import pickle\n",
    "import wiki_article \n",
    "import dictionary_definition\n",
    "import glob\n",
    "import modeling\n",
    "import itertools\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "from dataclasses import dataclass\n",
    "from io import StringIO\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "from scipy import stats\n",
    "import hashlib\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_checkpoints(base_dir):\n",
    "    checkpoint_dirs = glob.glob(f\"{base_dir}/checkpoint*\")\n",
    "    checkpoint_dirs.sort(key=lambda x: int(x[(x.index(\"checkpoint-\") + len(\"checkpoint-\")):]))\n",
    "    return checkpoint_dirs\n",
    "modeling_gpt\n",
    "def evaluate_lm_checkpoints(base_dir, validation_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    for d in get_checkpoints(base_dir):\n",
    "        model = AutoModelWithLMHead.from_pretrained(d).to('cuda')\n",
    "        refined_model_eval = wiki_article.lm_eval(model, tokenizer, validation_path)\n",
    "        print(f\"{d}: {refined_model_eval}\")\n",
    "tokenizer\n",
    "def evaluate_title_checkpoints(base_dir, validation_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")print(parsed_urban_dictionary_scraperpage.body.prettify())\n",
    "    for d in get_checkpoints(base_dir):\n",
    "        model = AutoModelWithLMHead.from_pretrained(d).to('cuda')\n",
    "        refined_model_eval = wiki_article.run_title_evaluation(model, tokenizer, validation_path)\n",
    "        print(f\"{d}: m={refined_model_eval.mean}, v={refined_model_eval.variance}\")\n",
    "\n",
    "# evaluate_lm_checkAutoModelWithLMHead, AutoTokenizer, points(\"models/wikitext_103_stride_512_v0/\", \"data/wikitext-103-title-train/wiki_title.valid.raw\")\n",
    "#print(glob.glob(\"models/wikitext_103_stride_512_v0/*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"data/en_dictionary_parsed_randomized.pickle\", \"rb\") as f:\n",
    "    parsed_dictionary = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens(datasets.SpecialTokens.special_tokens_dict())\n",
    "dataset = datasets.ParsedDictionaryDefinitionDataset(tokenizer, None, None, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|bod|> shanks' mare <|pos|> noun <|bd|> used to refer to one's own legs and the action of walking as a means of conveyance. <|eod|>\n"
     ]
    }
   ],
   "source": [
    "for example in dataset._make_examples(tokenizer, parsed_dictionary[1000]):\n",
    "    print(tokenizer.decode(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/all_words.pickle\", \"rb\") as f:\n",
    "    #words = pickle.load(f)\n",
    "    #items = list(words.items())\n",
    "    random.shuffle(items)\n",
    "    items = OrderedDict(items)\n",
    "    \n",
    "with open(\"data/all_words_randomized.pickle\", \"wb\") as f:\n",
    "    pickle.dump(items, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urban_dictionary_scraper.UrbanDictionaryDataset._make_examples(tokenizer, words[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"gpt2\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrefined_model_eval = wiki_article.run_title_evaluation(urban_dictionary_scrapermodel, tokenizer, \"wikitext-103-raw/wiki.valid.raw\")\n",
    "unrefined_model_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"output_103/\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_model_eval = wiki_article.run_title_evaluation(model, tokenizer, \"wikitext-103-raw/wiki.valid.raw\")\n",
    "refined_model_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = f\"\\\"TITLE\\\" is a song collaboration by Chinese artist Pamela Chen and Canadian singer Thomas Dimson, first released independently in March 2020. After gaining popularity amongst the cat community, the single was re-released by major label Columbia Records in May 2020. Pamela describes the song as being originally inspired by her two kittens, Apollo and Bean who once said meow.<bot>\"\n",
    "\n",
    "model =  modeling.GPT2LMHeadWithWeightedLossModel.from_pretrained(\"models/wikitext-103-raw-title-scale-20-lr5e-5\").to(\"cuda\")\n",
    "input = tokenizer.encode(sequence, return_tensors=\"pt\").to('cuda')\n",
    "generated = model.generate(input, max_length=100, num_return_sequences=100, temperature=1)\n",
    "\n",
    "print(f\"Prompt text: {sequence}\")\n",
    "for i in range(generated.size()[0]):\n",
    "    sentence_tokens = generated[i, :].tolist()\n",
    "    decoded = tokenizer.decode(sentence_tokens)\n",
    "    m = re.search(r\"<bot>(.*?)<eot>\", decoded)\n",
    "    if m:urban_dictionary_scraper\n",
    "        print(f\"{i}) {m.groups(1)}\")\n",
    "    else:\n",
    "        print(f\"{i}) Didn't work\")\n",
    "    \n",
    "\n",
    "resulting_string = tokenizer.decode(generated.tolist()[0])\n",
    "# print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in entries: \n",
    "    m = re.match(r\"\\s*\" + re.escape(entry.title) + r\"\\d*\\s*(\\|[^|]*\\|)?\\s*\", entry.entry_str)\n",
    "    if m:\n",
    "        trainable_entry = entry.entry_str[m.span()[1]:].strip()\n",
    "        if not trainable_entry:\n",
    "            raise RuntimeError(f\"Bad entry for {entry.title}: '{entry.entry_str}'\")\n",
    "    else:\n",
    "        raise RuntimeError(f\"Couldn't match {entry.title} on '{entry.entry_str}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_path = \"data/com_apple_MobileAsset_DictionaryServices_dictionaryOSX/69b7ab1cf0f75ad16bf6662b0a77fbfd36b7941f.asset/AssetData/New Oxford American Dictionary.dictionary/Contents/Resources/Body.data\"\n",
    "with open(dictionary_path, \"rb\") as f:\n",
    "    valid_words = {e.title.upper() for e in dictionary_definition.DictionaryDefinition.gen_from_apple_dictionary(f)}full_dataset = [\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  modeling.GPT2LMHeadWithWeightedLossModel.from_pretrained(\"models/dictionary-scale-10-lr5e-5\").to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = dictionary_definition.generate_words(\n",
    "    tokenizer, model, allow_proper_nouns=False, blacklist=valid_words, num=1000, max_iterations=40\n",
    ")\n",
    "words.sort(key=lambda x: x.title)\n",
    "for w in words:\n",
    "    print(f\"{w} {w.entry_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"words.tsv\", \"w\") as f:\n",
    "    for word in words:\n",
    "        f.write(f\"{word.title}\\t{word.entry_str}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens(datasets.SpecialTokens.special_tokens_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|bod|> Ghetto T-shirt Daddies <|bt|> An item worn by ghetto kids (the same ones that were wearing the Ghettos) that you buy on Walmart. Often wears a gator shirt or a sweatshirt. <|et|> I wish I could buy a ghetto t-shirt.\n",
      "\n",
      "Hey Ghetto T-shirt Daddies! <|eod|>\n"
     ]
    }
   ],
   "source": [
    "input = tokenizer.encode(f\"{tokenizer.bos_token}\", return_tensors=\"pt\").to(\"cuda\")\n",
    "max_length = 512\n",
    "\n",
    "generated = model.generate(\n",
    "    input_ids=input, \n",
    "    max_length=max_length, \n",
    "    num_return_sequences=5, \n",
    "    temperature=1.0,\n",
    "    top_p=0.92,\n",
    "    pad_token_id=tokenizer.pad_token_id,full_dataset = [\n",
    "    \n",
    "]\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_ids=tokenizer.eos_token_id,\n",
    "    do_sample=True,\n",
    ")\n",
    "sentence_tokens = generated[0, :].tolist()\n",
    "decoded = tokenizer.decode([e for e in sentence_tokens if e != tokenizer.pad_token_id])\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "blacklist = set(e.title for e in pickle.load(open(\"data/all_words.pickle\", \"rb\")).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generation: tried 350, failed 102 (0.29), no word in example 121 (0.49), filtered from blacklist 12 (0.09)\n"
     ]
    }
   ],
   "source": [
    "model =  modeling.GPT2LMHeadWithWeightedLossModel.from_pretrained(\n",
    "    \"models/urban_dictionary_cleaned_top_def_mu02_lr_0_000005_tw40\"\n",
    ").to(\"cuda\")\n",
    "tw40_words = urban_dictionary_scraper.generate_words(\n",
    "    tokenizer,\n",
    "    model,\n",
    "    blacklist=blacklist,\n",
    "    num=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tw1_words, open(\"data/labeling/tw1_words.pickle\", \"wb\"), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(tw40_words, open(\"data/labeling/tw40_words.pickle\", \"wb\"), protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    [\n",
    "        (\n",
    "            word.word,\n",
    "            word.definition,\n",
    "            word.example.replace(,\n",
    "            \"tw1\" if i < len(tw1_words) else \"tw2\",\n",
    "        )\n",
    "        for i, word in enumerate(itertools.chain(\n",
    "            tw1_words,\n",
    "            tw40_words\n",
    "        ))\n",
    "    ],\n",
    "    columns=(\"word\", \"definition\", \"example\", \"dataset\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_no_dataset = sample[:]\n",
    "sample_no_dataset.to_csv(\"fun.csv\", index=False, columns=[\"word\", \"definition\", \"example\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ipywidgets.widgets.interaction._InteractFactory at 0x7fe66fbe9f10>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
