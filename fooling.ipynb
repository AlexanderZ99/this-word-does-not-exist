{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "import sys\n",
    "import wiki_article \n",
    "import glob\n",
    "import modeling\n",
    "from dataclasses import dataclass\n",
    "from io import StringIO\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "interleave = 768\n",
    "wiki_article.generate_text_dataset(\n",
    "    open(\"wikitext-2-raw/wiki.train.raw\", \"r\"), \n",
    "    open(\"wikitext_title_train/wiki_title.train.raw\", \"w\"),\n",
    "    interleave=interleave,\n",
    ")\n",
    "wiki_article.generate_text_dataset(\n",
    "    open(\"wikitext-2-raw/wiki.valid.raw\", \"r\"), \n",
    "    open(\"wikitext_title_train/wiki_title.valid.raw\", \"w\"),\n",
    "    interleave=interleave,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/wikitext_103_stride_512_v0/checkpoint-500: 24.66460609436035\n",
      "models/wikitext_103_stride_512_v0/checkpoint-1000: 23.94955062866211\n",
      "models/wikitext_103_stride_512_v0/checkpoint-1500: 22.988603591918945\n",
      "models/wikitext_103_stride_512_v0/checkpoint-2000: 22.5709285736084\n",
      "models/wikitext_103_stride_512_v0/checkpoint-2500: 22.404022216796875\n",
      "models/wikitext_103_stride_512_v0/checkpoint-3000: 21.691320419311523\n",
      "models/wikitext_103_stride_512_v0/checkpoint-3500: 21.813074111938477\n",
      "models/wikitext_103_stride_512_v0/checkpoint-4000: 21.99334716796875\n",
      "models/wikitext_103_stride_512_v0/checkpoint-4500: 21.768508911132812\n",
      "models/wikitext_103_stride_512_v0/checkpoint-5000: 21.2707462310791\n",
      "models/wikitext_103_stride_512_v0/checkpoint-5500: 21.388479232788086\n",
      "models/wikitext_103_stride_512_v0/checkpoint-6000: 21.18572998046875\n",
      "models/wikitext_103_stride_512_v0/checkpoint-6500: 21.76972770690918\n",
      "models/wikitext_103_stride_512_v0/checkpoint-7000: 21.52361488342285\n",
      "models/wikitext_103_stride_512_v0/checkpoint-7500: 21.494911193847656\n",
      "models/wikitext_103_stride_512_v0/checkpoint-8000: 21.609756469726562\n",
      "models/wikitext_103_stride_512_v0/checkpoint-8500: 21.319339752197266\n",
      "models/wikitext_103_stride_512_v0/checkpoint-9000: 21.010173797607422\n",
      "models/wikitext_103_stride_512_v0/checkpoint-9500: 20.89507293701172\n",
      "models/wikitext_103_stride_512_v0/checkpoint-10000: 21.418249130249023\n",
      "models/wikitext_103_stride_512_v0/checkpoint-10500: 20.91526222229004\n",
      "models/wikitext_103_stride_512_v0/checkpoint-11000: 21.258888244628906\n",
      "models/wikitext_103_stride_512_v0/checkpoint-11500: 20.929290771484375\n",
      "models/wikitext_103_stride_512_v0/checkpoint-12000: 20.7396297454834\n",
      "models/wikitext_103_stride_512_v0/checkpoint-12500: 20.92534828186035\n",
      "models/wikitext_103_stride_512_v0/checkpoint-13000: 20.632261276245117\n",
      "models/wikitext_103_stride_512_v0/checkpoint-13500: 20.90458869934082\n",
      "models/wikitext_103_stride_512_v0/checkpoint-14000: 20.70326805114746\n",
      "models/wikitext_103_stride_512_v0/checkpoint-14500: 20.691335678100586\n",
      "models/wikitext_103_stride_512_v0/checkpoint-15000: 20.67804527282715\n",
      "models/wikitext_103_stride_512_v0/checkpoint-15500: 20.782222747802734\n",
      "models/wikitext_103_stride_512_v0/checkpoint-16000: 20.581829071044922\n",
      "models/wikitext_103_stride_512_v0/checkpoint-16500: 20.559999465942383\n",
      "models/wikitext_103_stride_512_v0/checkpoint-17000: 20.581642150878906\n",
      "models/wikitext_103_stride_512_v0/checkpoint-17500: 20.76776885986328\n",
      "models/wikitext_103_stride_512_v0/checkpoint-18000: 20.247997283935547\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0ddd577c9ec4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{d}: m={refined_model_eval.mean}, v={refined_model_eval.variance}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mevaluate_lm_checkpoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"models/wikitext_103_stride_512_v0/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data/wikitext-103-title-train/wiki_title.valid.raw\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;31m#print(glob.glob(\"models/wikitext_103_stride_512_v0/*\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-0ddd577c9ec4>\u001b[0m in \u001b[0;36mevaluate_lm_checkpoints\u001b[0;34m(base_dir, validation_path)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_checkpoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelWithLMHead\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mrefined_model_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwiki_article\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{d}: {refined_model_eval}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/company-makeup/wiki_article.py\u001b[0m in \u001b[0;36mlm_eval\u001b[0;34m(model, tokenizer, file_path, device, block_size, batch_size)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mtokenized_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_text\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mblock_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Truncate in block of block_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_inputs_with_special_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mblock_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0madded_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_added_tokens_encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m         \u001b[0mtokenized_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_on_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madded_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36msplit_on_tokens\u001b[0;34m(tok_list, text)\u001b[0m\n\u001b[1;32m    804\u001b[0m                     (\n\u001b[1;32m    805\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_added_tokens_encoder\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m                         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m                     )\n\u001b[1;32m    808\u001b[0m                 )\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    804\u001b[0m                     (\n\u001b[1;32m    805\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_added_tokens_encoder\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m                         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m                     )\n\u001b[1;32m    808\u001b[0m                 )\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/transformers/tokenization_gpt2.py\u001b[0m in \u001b[0;36m_tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyte_encoder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             )  # Maps all our bytes to unicode strings, avoiding controle tokens of the BPE (spaces in our case)\n\u001b[0;32m--> 228\u001b[0;31m             \u001b[0mbpe_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbpe_token\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbpe_token\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbpe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbpe_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/transformers/tokenization_gpt2.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyte_encoder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             )  # Maps all our bytes to unicode strings, avoiding controle tokens of the BPE (spaces in our case)\n\u001b[0;32m--> 228\u001b[0;31m             \u001b[0mbpe_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbpe_token\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbpe_token\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbpe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbpe_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_checkpoints(base_dir):\n",
    "    checkpoint_dirs = glob.glob(f\"{base_dir}/checkpoint*\")\n",
    "    checkpoint_dirs.sort(key=lambda x: int(x[(x.index(\"checkpoint-\") + len(\"checkpoint-\")):]))\n",
    "    return checkpoint_dirs\n",
    "modeling_gpt\n",
    "def evaluate_lm_checkpoints(base_dir, validation_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    for d in get_checkpoints(base_dir):\n",
    "        model = AutoModelWithLMHead.from_pretrained(d).to('cuda')\n",
    "        refined_model_eval = wiki_article.lm_eval(model, tokenizer, validation_path)\n",
    "        print(f\"{d}: {refined_model_eval}\")\n",
    "\n",
    "def evaluate_title_checkpoints(base_dir, validation_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    for d in get_checkpoints(base_dir):\n",
    "        model = AutoModelWithLMHead.from_pretrained(d).to('cuda')\n",
    "        refined_model_eval = wiki_article.run_title_evaluation(model, tokenizer, validation_path)\n",
    "        print(f\"{d}: m={refined_model_eval.mean}, v={refined_model_eval.variance}\")\n",
    "\n",
    "# evaluate_lm_checkpoints(\"models/wikitext_103_stride_512_v0/\", \"data/wikitext-103-title-train/wiki_title.valid.raw\")\n",
    "#print(glob.glob(\"models/wikitext_103_stride_512_v0/*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"gpt2\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DescribeResult(nobs=60, minmax=(9.851899147033691, 15.262736320495605), mean=12.927889172236124, variance=1.572701685821396, skewness=-0.44557272655893687, kurtosis=-0.35688076179019035)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unrefined_model_eval = wiki_article.run_title_evaluation(model, tokenizer, \"wikitext-103-raw/wiki.valid.raw\")\n",
    "unrefined_model_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"output_103/\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DescribeResult(nobs=60, minmax=(10.398584365844727, 17.0587158203125), mean=13.77767310142517, variance=1.9294105073945371, skewness=-0.01741720699601202, kurtosis=-0.0981178051223095)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refined_model_eval = wiki_article.run_title_evaluation(model, tokenizer, \"wikitext-103-raw/wiki.valid.raw\")\n",
    "refined_model_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt text: \"TITLE\" is a song collaboration by Chinese artist Pamela Chen and Canadian singer Thomas Dimson, first released independently in March 2020. After gaining popularity amongst the cat community, the single was re-released by major label Columbia Records in May 2020. Pamela describes the song as being originally inspired by her two kittens, Apollo and Bean who once said meow.<bot>\n",
      "0) ('Animal Heart',)\n",
      "1) ('The Song of a Cuddle',)\n",
      "2) ('Letting You Come',)\n",
      "3) ('Escape from Cats',)\n",
      "4) (\"So You Think It's All Good\",)\n",
      "5) (\"Don 't Worry About You\",)\n",
      "6) ('I Am... TITLE',)\n",
      "7) ('I Put a Smile on a Smile Baby',)\n",
      "8) ('I Wanna Be',)\n",
      "9) ('Pump the Pump',)\n"
     ]
    }
   ],
   "source": [
    "sequence = f\"\\\"TITLE\\\" is a song collaboration by Chinese artist Pamela Chen and Canadian singer Thomas Dimson, first released independently in March 2020. After gaining popularity amongst the cat community, the single was re-released by major label Columbia Records in May 2020. Pamela describes the song as being originally inspired by her two kittens, Apollo and Bean who once said meow.<bot>\"\n",
    "\n",
    "input = tokenizer.encode(sequence, return_tensors=\"pt\").to('cuda')\n",
    "generated = model.generate(input, max_length=100, num_return_sequences=10, temperature=1)\n",
    "\n",
    "print(f\"Prompt text: {sequence}\")\n",
    "for i in range(generated.size()[0]):\n",
    "    sentence_tokens = generated[i, :].tolist()\n",
    "    decoded = tokenizer.decode(sentence_tokens)\n",
    "    m = re.search(r\"<bot>(.modeling_gpt*?)<eot>\", decoded)\n",
    "    if m:\n",
    "        print(f\"{i}) {m.groups(1)}\")\n",
    "    else:\n",
    "        print(f\"{i}) Didn't work\")\n",
    "    \n",
    "\n",
    "resulting_string = tokenizer.decode(generated.tolist()[0])\n",
    "# print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = modeling.GPT2LMHeadWithWeightedLossModel.from_pretrained(\"gpt2\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-6.2348, device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_weights = torch.tensor(\n",
    "    [\n",
    "        [1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "        [1.0, 1.0, 0.7, 1.0, 0.0, 1.0],\n",
    "        [1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "        [1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "    ]\n",
    ").to('cuda')\n",
    "wiki_article.perplexity(model, tokenizer, [\"abc\", \"abc\", \"bbq\", \"abc abc really long sentence\"], loss_weights=loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39305"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.build_inputs_with_special_tokens(tokenizer.convert_tokens_to_ids(\"abc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([11246, 2420, 27, 7839, 29, 12543, 1748, 3556, 7839, 29, 124314],\n",
       " [False, False, False, True, True, True, True, True, True, True, False])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"some text\"))\n",
    "token_title = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"<title>fun city</title>\"))\n",
    "wiki_article.ArticleTitleDataset._make_example(tokenizer, token_text, token_title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
