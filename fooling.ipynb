{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urban_dictionary_scraper\n",
    "import torch\n",
    "import re\n",
    "import sys\n",
    "import pickle\n",
    "import wiki_article \n",
    "import dictionary_definition\n",
    "import glob\n",
    "import modeling\n",
    "import itertools\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from io import StringIO\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "from scipy import stats\n",
    "import hashlib\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_checkpoints(base_dir):\n",
    "    checkpoint_dirs = glob.glob(f\"{base_dir}/checkpoint*\")\n",
    "    checkpoint_dirs.sort(key=lambda x: int(x[(x.index(\"checkpoint-\") + len(\"checkpoint-\")):]))\n",
    "    return checkpoint_dirs\n",
    "modeling_gpt\n",
    "def evaluate_lm_checkpoints(base_dir, validation_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    for d in get_checkpoints(base_dir):\n",
    "        model = AutoModelWithLMHead.from_pretrained(d).to('cuda')\n",
    "        refined_model_eval = wiki_article.lm_eval(model, tokenizer, validation_path)\n",
    "        print(f\"{d}: {refined_model_eval}\")\n",
    "tokenizer\n",
    "def evaluate_title_checkpoints(base_dir, validation_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")print(parsed_page.body.prettify())\n",
    "    for d in get_checkpoints(base_dir):\n",
    "        model = AutoModelWithLMHead.from_pretrained(d).to('cuda')\n",
    "        refined_model_eval = wiki_article.run_title_evaluation(model, tokenizer, validation_path)\n",
    "        print(f\"{d}: m={refined_model_eval.mean}, v={refined_model_eval.variance}\")\n",
    "\n",
    "# evaluate_lm_checkAutoModelWithLMHead, AutoTokenizer, points(\"models/wikitext_103_stride_512_v0/\", \"data/wikitext-103-title-train/wiki_title.valid.raw\")\n",
    "#print(glob.glob(\"models/wikitext_103_stride_512_v0/*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens(urban_dictionary_scraper.SpecialTokens.special_tokens_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/all_words.pickle\", \"rb\") as f:\n",
    "    #words = pickle.load(f)\n",
    "    #items = list(words.items())\n",
    "    random.shuffle(items)\n",
    "    items = OrderedDict(items)\n",
    "    \n",
    "with open(\"data/all_words_randomized.pickle\", \"wb\") as f:\n",
    "    pickle.dump(items, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urban_dictionary_scraper.UrbanDictionaryDataset._make_examples(tokenizer, words[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"gpt2\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrefined_model_eval = wiki_article.run_title_evaluation(model, tokenizer, \"wikitext-103-raw/wiki.valid.raw\")\n",
    "unrefined_model_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"output_103/\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_model_eval = wiki_article.run_title_evaluation(model, tokenizer, \"wikitext-103-raw/wiki.valid.raw\")\n",
    "refined_model_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = f\"\\\"TITLE\\\" is a song collaboration by Chinese artist Pamela Chen and Canadian singer Thomas Dimson, first released independently in March 2020. After gaining popularity amongst the cat community, the single was re-released by major label Columbia Records in May 2020. Pamela describes the song as being originally inspired by her two kittens, Apollo and Bean who once said meow.<bot>\"\n",
    "\n",
    "model =  modeling.GPT2LMHeadWithWeightedLossModel.from_pretrained(\"models/wikitext-103-raw-title-scale-20-lr5e-5\").to(\"cuda\")\n",
    "input = tokenizer.encode(sequence, return_tensors=\"pt\").to('cuda')\n",
    "generated = model.generate(input, max_length=100, num_return_sequences=100, temperature=1)\n",
    "\n",
    "print(f\"Prompt text: {sequence}\")\n",
    "for i in range(generated.size()[0]):\n",
    "    sentence_tokens = generated[i, :].tolist()\n",
    "    decoded = tokenizer.decode(sentence_tokens)\n",
    "    m = re.search(r\"<bot>(.*?)<eot>\", decoded)\n",
    "    if m:\n",
    "        print(f\"{i}) {m.groups(1)}\")\n",
    "    else:\n",
    "        print(f\"{i}) Didn't work\")\n",
    "    \n",
    "\n",
    "resulting_string = tokenizer.decode(generated.tolist()[0])\n",
    "# print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in entries: \n",
    "    m = re.match(r\"\\s*\" + re.escape(entry.title) + r\"\\d*\\s*(\\|[^|]*\\|)?\\s*\", entry.entry_str)\n",
    "    if m:\n",
    "        trainable_entry = entry.entry_str[m.span()[1]:].strip()\n",
    "        if not trainable_entry:\n",
    "            raise RuntimeError(f\"Bad entry for {entry.title}: '{entry.entry_str}'\")\n",
    "    else:\n",
    "        raise RuntimeError(f\"Couldn't match {entry.title} on '{entry.entry_str}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_path = \"data/com_apple_MobileAsset_DictionaryServices_dictionaryOSX/69b7ab1cf0f75ad16bf6662b0a77fbfd36b7941f.asset/AssetData/New Oxford American Dictionary.dictionary/Contents/Resources/Body.data\"\n",
    "with open(dictionary_path, \"rb\") as f:\n",
    "    valid_words = {e.title.upper() for e in dictionary_definition.DictionaryDefinition.gen_from_apple_dictionary(f)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  modeling.GPT2LMHeadWithWeightedLossModel.from_pretrained(\"models/dictionary-scale-10-lr5e-5\").to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = dictionary_definition.generate_words(\n",
    "    tokenizer, model, allow_proper_nouns=False, blacklist=valid_words, num=1000, max_iterations=40\n",
    ")\n",
    "words.sort(key=lambda x: x.title)\n",
    "for w in words:\n",
    "    print(f\"{w} {w.entry_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"words.tsv\", \"w\") as f:\n",
    "    for word in words:\n",
    "        f.write(f\"{word.title}\\t{word.entry_str}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
