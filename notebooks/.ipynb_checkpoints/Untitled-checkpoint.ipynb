{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(\"../title_maker_pro\")\n",
    "sys.path.append(\"../website\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 116kB [00:00, 9.53MB/s]                    \n",
      "2020-05-04 23:38:13 INFO: Downloading default packages for language: en (English)...\n",
      "2020-05-04 23:38:14 INFO: File exists: /home/tdimson/stanza_resources/en/default.zip.\n",
      "2020-05-04 23:38:17 INFO: Finished downloading models and saved to /home/tdimson/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import stanza\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import datasets\n",
    "import pickle\n",
    "import torch\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "stanza.download('en')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_words(words, f):\n",
    "    for word in words:\n",
    "        word_str = [word.word]\n",
    "        if word.pos:\n",
    "            word_str.append(f\"/{word.pos}/\")\n",
    "        if word.topic:\n",
    "            word_str.append(f\"[{word.topic}]\")\n",
    "        print(\" \".join(word_str), file=f)\n",
    "        print(f\"\\t{word.definition}{' |n| ' if word.example is None else ''}\", file=f)\n",
    "        if word.example:\n",
    "            print(f\"\\t\\\"{word.example}\\\"{' |e|' if word.from_example_expansion else ''}\", file=f)\n",
    "\n",
    "        print(\"\", file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-04 23:45:18 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
      "2020-05-04 23:45:18 INFO: Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ewt     |\n",
      "| pos       | ewt     |\n",
      "=======================\n",
      "\n",
      "2020-05-04 23:45:18 INFO: Use device: gpu\n",
      "2020-05-04 23:45:18 INFO: Loading: tokenize\n",
      "2020-05-04 23:45:18 INFO: Loading: pos\n",
      "2020-05-04 23:45:19 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens(datasets.SpecialTokens.special_tokens_dict())\n",
    "blacklist = datasets.Blacklist.load(\"../build/blacklist.pickle\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"/mnt/evo/projects/title-maker-pro/models/en_dictionary_parsed_lr_00001_creativity/checkpoint-120000/\").to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit datasets.ParsedDictionaryDefinitionDataset.evaluate_creativity(tokenizer, model, blacklist, 100, 50, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_weird(w):\n",
    "    return (\n",
    "            w.word[-1] != \"-\"\n",
    "            and \"<|\" not in w.definition\n",
    "            and \"<|\" not in w.example\n",
    "            and (not w.pos or \"<|\" not in w.pos)\n",
    "            and len(w.word.split()) <= 3\n",
    "        )\n",
    "         \n",
    "def go(**kwargs):\n",
    "    return datasets.ParsedDictionaryDefinitionDataset.generate_words(\n",
    "        tokenizer, model,\n",
    "        num=20,\n",
    "        max_iterations=5, \n",
    "        blacklist=blacklist, \n",
    "        do_example_expansion=False, \n",
    "        generation_args=dict(\n",
    "            top_k=200,\n",
    "            num_return_sequences=50,\n",
    "            max_length=375,\n",
    "            do_sample=True,\n",
    "        ),\n",
    "        expansion_generation_overrides=dict(\n",
    "            top_k=50,\n",
    "            num_return_sequences=25,\n",
    "            do_sample=True,\n",
    "        ),\n",
    "        num_expansion_candidates=25,\n",
    "        filter_proper_nouns=True,\n",
    "        user_filter=no_weird,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "# words, stats = go()\n",
    "# print(stats)\n",
    "# print()\n",
    "# print_words(words, sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "missing example???\n",
      "4.01 s ± 2.29 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit go(use_custom_generate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.31 s ± 916 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit go(use_custom_generate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blacklist.contains(\"foolage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_words(words[:100], sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from transformers import activations\n",
    "import transformers\n",
    "\n",
    "def gelu_new(x):\n",
    "    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
    "\n",
    "activations.ACT2FN['gelu_new'] = gelu_new\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"../build/forward-dictionary-model-v1\").to(\"cpu\")\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear, torch.nn.Embedding, transformers.modeling_utils.Conv1D}, dtype=torch.qint8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = go2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(a[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit go2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from words import WordIndex, Word\n",
    "def clean_example(w, example):\n",
    "    return re.sub(w, w, example, flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "wi = WordIndex(\n",
    "    [\n",
    "        Word(\n",
    "            word=w.word,\n",
    "            definition=w.definition,\n",
    "            pos=w.pos,\n",
    "            topic=w.topic,\n",
    "            example=clean_example(w.word, w.example),\n",
    "        ) for w in words\n",
    "    ]\n",
    ")\n",
    "wi.dump(\"../website/data/words2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyphen import Hyphenator\n",
    "h_en = Hyphenator('en_US')\n",
    "h_en.syllables('fancccwe')\n",
    "wi2 = WordIndex.load(\"../website/data/words.json\")\n",
    "wi_p = WordIndex(\n",
    "    [\n",
    "        Word(\n",
    "            word=w.word,\n",
    "            definition=w.definition,\n",
    "            pos=w.pos,\n",
    "            topic=w.topic,\n",
    "            example=clean_example(w.word, w.example),\n",
    "            syllables=h_en.syllables(w.word)\n",
    "        )\n",
    "        for w in wi2.words\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wi_p.dump(\"../website/data/words.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "company_makeup",
   "language": "python",
   "name": "company_makeup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
