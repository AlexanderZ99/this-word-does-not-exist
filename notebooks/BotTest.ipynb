{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import torch\n",
    "from title_maker_pro.word_generator import WordGenerator\n",
    "import datasets\n",
    "import re\n",
    "\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 116kB [00:00, 5.99MB/s]                    \n",
      "INFO:stanza:Downloading default packages for language: en (English)...\n",
      "INFO:stanza:File exists: /home/tdimson/stanza_resources/en/default.zip.\n",
      "INFO:stanza:Finished downloading models and saved to /home/tdimson/stanza_resources.\n",
      "WARNING:stanza:Can not find mwt: default from official model list. Ignoring it.\n",
      "INFO:stanza:Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ewt     |\n",
      "| pos       | ewt     |\n",
      "=======================\n",
      "\n",
      "INFO:stanza:Use device: cpu\n",
      "INFO:stanza:Loading: tokenize\n",
      "INFO:stanza:Loading: pos\n",
      "INFO:stanza:Done loading processors!\n",
      "INFO:title_maker_pro.word_generator:Using device cpu\n",
      "INFO:title_maker_pro.word_generator:Loading word blacklist from /mnt/evo/projects/title-maker-pro/models/blacklist.pickle...\n",
      "INFO:title_maker_pro.word_generator:Loaded 5763399 words to blacklist\n",
      "INFO:title_maker_pro.word_generator:Loading GPT2 tokenizer...\n",
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /home/tdimson/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.4c1d7fc2ac6ddabeaf0c8bec2ffc7dc112f668f5871a06efcff113d2797ec7d5\n",
      "INFO:transformers.configuration_utils:Model config GPT2Config {\n",
      "  \"_num_labels\": 2,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /home/tdimson/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /home/tdimson/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "INFO:transformers.tokenization_utils:Adding <|bod|> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Assigning <|bod|> to the bos_token key of the tokenizer\n",
      "INFO:transformers.tokenization_utils:Adding <|eod|> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Assigning <|eod|> to the eos_token key of the tokenizer\n",
      "INFO:transformers.tokenization_utils:Adding <|pad|> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Assigning <|pad|> to the pad_token key of the tokenizer\n",
      "INFO:transformers.tokenization_utils:Adding <|bd|> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Adding <|be|> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Adding <|pos|> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Adding <|bto|> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Assigning ['<|bd|>', '<|be|>', '<|pos|>', '<|bto|>'] to the additional_special_tokens key of the tokenizer\n",
      "INFO:title_maker_pro.word_generator:Loaded tokenizer\n",
      "INFO:title_maker_pro.word_generator:Peforming quantization on models\n",
      "INFO:title_maker_pro.word_generator:Loading forward model from /mnt/evo/projects/title-maker-pro/models/en_dictionary_parsed_lr_00001/checkpoint-120000/\n",
      "INFO:transformers.configuration_utils:loading configuration file /mnt/evo/projects/title-maker-pro/models/en_dictionary_parsed_lr_00001/checkpoint-120000/config.json\n",
      "INFO:transformers.configuration_utils:Model config GPT2Config {\n",
      "  \"_num_labels\": 2,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"eos_token_ids\": [\n",
      "    50256\n",
      "  ],\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file /mnt/evo/projects/title-maker-pro/models/en_dictionary_parsed_lr_00001/checkpoint-120000/pytorch_model.bin\n",
      "INFO:title_maker_pro.word_generator:Loaded forward model\n",
      "INFO:title_maker_pro.word_generator:Loading inverse model from /mnt/evo/projects/title-maker-pro/models/inverse_en_dictionary_parsed_lr_00001/checkpoint-50000/\n",
      "INFO:transformers.configuration_utils:loading configuration file /mnt/evo/projects/title-maker-pro/models/inverse_en_dictionary_parsed_lr_00001/checkpoint-50000/config.json\n",
      "INFO:transformers.configuration_utils:Model config GPT2Config {\n",
      "  \"_num_labels\": 2,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file /mnt/evo/projects/title-maker-pro/models/inverse_en_dictionary_parsed_lr_00001/checkpoint-50000/pytorch_model.bin\n",
      "INFO:title_maker_pro.word_generator:Loaded inverse model\n"
     ]
    }
   ],
   "source": [
    "wg = WordGenerator(\n",
    "    forward_model_path=\"/mnt/evo/projects/title-maker-pro/models/en_dictionary_parsed_lr_00001/checkpoint-120000/\",\n",
    "    inverse_model_path=\"/mnt/evo/projects/title-maker-pro/models/inverse_en_dictionary_parsed_lr_00001/checkpoint-50000/\",\n",
    "    blacklist_path=\"/mnt/evo/projects/title-maker-pro/models/blacklist.pickle\",\n",
    "    device='cpu',\n",
    "    quantize=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:title_maker_pro.word_generator:Generation stats: iterations=1 time=1.42510986328125 stanza_time=0.0 | items_considered 1.00@5, failed_match 0.00@0, blacklist_filtered 0.00@0, seen_filtered 0.00@0, proper_noun_filtered 0.00@0, example_missing 0.00@0, example_missing_title 1.00@5, example_pos_match_failed 0.00@0, user_filtered 0.00@0, returned 0.00@0 (found 0 true and 5 viable)\n",
      "INFO:title_maker_pro.word_generator:No candidate has title in example, doing hail mary inference\n",
      "INFO:title_maker_pro.word_generator:Hail mary stats: iterations=1 time=0.2498178482055664 stanza_time=0.0 | items_considered 1.00@1, failed_match 0.00@0, blacklist_filtered 0.00@0, seen_filtered 0.00@0, proper_noun_filtered 0.00@0, example_missing 0.00@0, example_missing_title 0.00@0, example_pos_match_failed 0.00@0, user_filtered 0.00@0, returned 1.00@1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GeneratedWord(word='$#!Caf325', pos='abbreviation', topic=None, definition=\"carbon copy letter if you don't find it in the original paper.\", example='$#!Caf325 is the standard version for most printers', decoded=\"<|bod|> $#!Caf325 <|pos|> abbreviation <|bd|> carbon copy letter if you don't find it in the original paper. <|be|> $#!Caf325 is the standard version for most printers <|eod|>\", decoded_tokens=[50257, 3, 2, 0, 34, 1878, 26582, 50262, 397, 4679, 47625, 50260, 29255, 4866, 3850, 611, 345, 836, 470, 1064, 340, 287, 262, 2656, 3348, 13, 50261, 3, 2, 0, 34, 1878, 26582, 318, 262, 3210, 2196, 329, 749, 34654, 50258])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wg.generate_definition(\"$#!Caf325\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:twitter_bot:Generation stats: iterations=1 | items_considered 1.00@1, failed_match 0.00@0, blacklist_filtered 0.00@0, seen_filtered 0.00@0, proper_noun_filtered 0.00@0, example_missing 0.00@0, example_missing_title 0.00@0, example_pos_match_failed 0.00@0, user_filtered 0.00@0, returned 1.00@1\n",
      "INFO:twitter_bot:Word generation (forward) took 2.32s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.8 s, sys: 339 ms, total: 37.1 s\n",
      "Wall time: 2.32 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'@w 👉 cleanecode /noun/ \\na machine for generating clean code in assembler. \\n\"the compiler provides easy assembly procedures using cleanecodes\"'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time twitter_bot._formulate_reply_text(wg, \"cleanecode\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@w 👉 divasiveness: the feeling of dying slowly'"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_bot._formulate_reply_text(wg, \"the feeling of dying slowly\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wg.blacklist.contains(\"swapin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bak'"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r\"('s|s|ing)$\", \"\", \"baking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParsedDictionaryDefinitionDataset.GeneratedWord(word='hello world', pos='noun', topic=None, definition='an amusing or impressive pastime or object', example='hello world golfing', from_example_expansion=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wg.generate_definition(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' funnster ',\n",
       " None,\n",
       " None,\n",
       " ' verb [no object] <|bd|> have fun together ',\n",
       " \" there's an abundance of people who funnster around at his apartment \")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = \"<|bod|> funnster <|bd|> verb [no object] <|bd|> have fun together <|be|> he put on a lively little show at the pub <|be|> there's an abundance of people who funnster around at his apartment <|eod|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|>\"\n",
    "s = datasets.ParsedDictionaryDefinitionDataset._split_re()\n",
    "s.match(e).groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@coolman123 hello test 123 /exclamation/ used to introduce the reply of someone who comes into the room to ask you to believe something. \"hello test 123, what happened to your dog?\"'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_bot._formulate_reply_text(wg, \"@WordMakerPro define hello test 123\", \"coolman123\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "company_makeup",
   "language": "python",
   "name": "company_makeup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
