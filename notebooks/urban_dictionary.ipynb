{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import itertools \n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../title_maker_pro\")\n",
    "from title_maker_pro import datasets\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "import copy\n",
    "from word_generator import WordGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/mnt/evo/projects/title-maker-pro/data/urban_dictionary_words.pickle\"\n",
    "with open(dataset_path, 'rb') as f:\n",
    "    dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title_maker_pro.datasets.Blacklist at 0x7fcd21abea50>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blacklist = datasets.Blacklist.load(\"/mnt/evo/projects/title-maker-pro/models/blacklist.pickle\")\n",
    "blacklist.merge(\n",
    "    datasets.Blacklist.from_text_lines(\"/mnt/evo/projects/title-maker-pro/names.txt\")\n",
    ")\n",
    "blacklist.merge(\n",
    "    datasets.Blacklist.from_text_lines(\"/mnt/evo/projects/title-maker-pro/names2.txt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_defs = {k.lower(): len(d.definitions) for k, d in dataset.items()}\n",
    "rows = []\n",
    "seen_set = set()\n",
    "for i, d in sorted(enumerate(itertools.chain.from_iterable(e.definitions for e in dataset.values())), key=lambda x: x[1].upvotes, reverse=True):\n",
    "    highest_ranked_def = d.word.lower() not in seen_set\n",
    "    rows.append((i, d.word, d.meaning, d.examples[0], d.upvotes, d.downvotes, d.creation_epoch, num_defs.get(d.word.lower(), 0), highest_ranked_def))\n",
    "    seen_set.add(d.word.lower())\n",
    "                                                \n",
    "pd_dataset = pd.DataFrame(\n",
    "    rows,\n",
    "    columns=[\"idx\", \"word\", \"meaning\", \"example\", \"upvotes\", \"downvotes\", \"creation_epoch\", \"num_defs\", \"highest_rank_def\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(original, f, name):\n",
    "    n = original[f]\n",
    "    print(f\"{name} cut by {100 * (1 - (len(n) / (len(original)))):.2f}% ({len(original)} -> {len(n)})\")\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probably_name_meaning(meaning):\n",
    "    m = re.search(r\"(^|\\b)(boy|girl)\", meaning)\n",
    "    if m:\n",
    "        return m.start() < 20\n",
    "    \n",
    "    return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blacklist cut by 50.37% (2961824 -> 1469932)\n",
      "min_definitions cut by 67.75% (1469932 -> 474071)\n",
      "only_best_def cut by 61.28% (474071 -> 183568)\n",
      "max_words cut by 0.68% (183568 -> 182319)\n",
      "min_len cut by 2.39% (182319 -> 177955)\n",
      "length cut by 36.53% (177955 -> 112955)\n",
      "name_definitions cut by 3.34% (112955 -> 109178)\n"
     ]
    }
   ],
   "source": [
    "t = pd_dataset.copy()\n",
    "# t[\"upvote_percentage\"] = t[\"upvotes\"] / (t[\"upvotes\"] + t[\"downvotes\"] + 5)\n",
    "t = cut(t, ~(t[\"word\"].apply(blacklist.contains)), name=\"blacklist\")\n",
    "t = cut(t, (t[\"num_defs\"] >= 2), name=\"min_definitions\")\n",
    "t = cut(t, t[\"highest_rank_def\"], name=\"only_best_def\")\n",
    "#t = cut(t, ~(t[\"word\"].apply(blacklist.contains)), name=\"blacklist\")\n",
    "#t = cut(t, ~(t[\"word\"].apply(lambda x: x[:1].isupper())), name=\"uppercase\")\n",
    "t = cut(t, (t[\"word\"].apply(lambda x: len(x.split()) <= 3)), name=\"max_words\")\n",
    "t = cut(t, (t[\"word\"].str.len() >= 4), name=\"min_len\")\n",
    "t = cut(t, ((t[\"example\"].str.len() + t[\"meaning\"].str.len() + t[\"word\"].str.len()) < 250), name=\"length\")\n",
    "t = cut(t, ~(t[\"meaning\"].apply(probably_name_meaning)), name=\"name_definitions\")\n",
    "\n",
    "#t = cut(t, (t[\"upvote_percentage\"] >= 0.5), name=\"upvote_percentage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_indexes = set(t[\"idx\"])\n",
    "cleaned_dataset = OrderedDict()\n",
    "i = 0\n",
    "num_defns = 0\n",
    "for k, ud_word in dataset.items():\n",
    "    good_defns = []\n",
    "    for d in ud_word.definitions:\n",
    "        if i in valid_indexes:\n",
    "            good_defns.append(copy.deepcopy(d))\n",
    "            num_defns += 1\n",
    "        i += 1\n",
    "    \n",
    "    if good_defns:\n",
    "        new = copy.deepcopy(ud_word)\n",
    "        for defn in good_defns:\n",
    "            if sum(1 for c in defn.word if c.isupper()) == 1:\n",
    "                defn.word = defn.word.lower()\n",
    "        new.definitions = good_defns\n",
    "        cleaned_dataset[k] = new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataset_path = \"/mnt/evo/projects/title-maker-pro/data/urban_dictionary_250_top_defs.pickle\"\n",
    "with open(cleaned_dataset_path, \"wb\") as f:\n",
    "    pickle.dump(cleaned_dataset, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos', use)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens(datasets.SpecialTokens.special_tokens_dict())\n",
    "blacklist = datasets.Blacklist.load(\"/mnt/evo/projects/title-maker-pro/models/blacklist_urban_dictionary.pickle\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"/mnt/evo/projects/title-maker-pro/models/urban_dictionary_250_cleaned_top_defs_lr_00002_b9/checkpoint-50000\").to(\"cuda:0\")# model = AutoModelWithLMHead.from_pretrained(\"/mnt/evo/projects/title-maker-pro/models/urban_dictionary_250_cleaned_lr_00005_b9_seed4/checkpoint-140000\").to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f205295f1ba5490cab29cc2d598c8b17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words, stats = datasets.UrbanDictionaryDataset.generate_words(\n",
    "    tokenizer, model,\n",
    "    num=20000,\n",
    "    max_iterations=1000, \n",
    "    blacklist=blacklist, \n",
    "    generation_args=dict(\n",
    "        top_k=50,\n",
    "        num_return_sequences=250,\n",
    "        max_length=250,\n",
    "        do_sample=True,\n",
    "    ),\n",
    "    dedupe_titles=True,\n",
    "    filter_proper_nouns=False,\n",
    "    min_definition_words=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from title_maker_pro.bad_words import ULTRA_BAD_REGEX\n",
    "from website.words import WordIndex, Word\n",
    "from word_service.word_service_proto import wordservice_pb2\n",
    "\n",
    "def clean_example(w, example):\n",
    "    return re.sub(re.escape(w), w, example, flags=re.IGNORECASE)\n",
    "\n",
    "def word_filter(words):\n",
    "    filters = defaultdict(int)\n",
    "    ret = []\n",
    "    def run_over_all_text(pat, word):\n",
    "        return (\n",
    "            re.search(pat, word.word.strip(), flags=re.IGNORECASE) \n",
    "            or re.search(pat, word.definition.strip(), flags=re.IGNORECASE) \n",
    "            or re.search(pat, word.example.strip(), flags=re.IGNORECASE)\n",
    "        )\n",
    "    \n",
    "    for word in words:\n",
    "        if re.search(r\"(^|\\b)nig+\", word.word.strip()):\n",
    "            filters[\"nig\"] += 1\n",
    "        elif re.search(r\"(^|\\b)mex+\", word.word.strip()):\n",
    "            filters[\"mex\"] += 1\n",
    "        elif run_over_all_text(r\"(\\b|^)fagg+ots*\", word):\n",
    "            filters[\"fggot\"] += 1\n",
    "        elif run_over_all_text(r\"(\\b|^)f+a+g+\", word):\n",
    "            filters[\"fg\"] += 1\n",
    "        elif run_over_all_text(r\"ghettos?\", word):\n",
    "            filters[\"ghetto\"] += 1\n",
    "        elif run_over_all_text(r\"skanks*\", word):\n",
    "            filters[\"sknk\"] += 1\n",
    "        elif run_over_all_text(r\"(^|\\b)p+a+k+i+(\\b|$)\", word):\n",
    "            filters[\"pki\"]\n",
    "        elif run_over_all_text(r\"(^|\\b)cunt+\", word):\n",
    "            filters[\"cnt\"] += 1\n",
    "        elif run_over_all_text(r\"(^|\\b)indian($|\\b)\", word):\n",
    "            filters['indian'] += 1\n",
    "        elif run_over_all_text(r\"c+h+i+n+k+\", word):\n",
    "            filters['chnk'] += 1\n",
    "        elif run_over_all_text(r\"nigga+s*\", word):\n",
    "            filters['ngga'] += 1\n",
    "        elif run_over_all_text(r\"(^|\\b)slap+s*(^|\\b)\", word):\n",
    "            filters['slap'] += 1\n",
    "        elif run_over_all_text(r\"(^|\\b)r+a+p+e+s*(^|\\b)\", word):\n",
    "            filters['rape'] += 1\n",
    "        elif ULTRA_BAD_REGEX.search(word.word.strip()):\n",
    "            filters[\"ultra_bad_word\"] += 1\n",
    "        elif ULTRA_BAD_REGEX.search(word.definition.strip()):\n",
    "            filters[\"ultra_bad_def\"] += 1\n",
    "        elif ULTRA_BAD_REGEX.search(word.example.strip()):\n",
    "            filters[\"ultra_bad_example\"] += 1\n",
    "        else:\n",
    "            ret.append(word)\n",
    "            \n",
    "    for k,v in sorted(filters.items()):\n",
    "        print(f\"Filter '{k}' removed {100 * v / len(words):.2f}%\")\n",
    "        \n",
    "    print(f\"Total removed {100 * (1 - len(ret) / len(words)):.2f}%\")\n",
    "        \n",
    "    return ret\n",
    "    \n",
    "from hyphen import Hyphenator\n",
    "h_en = Hyphenator('en_US')\n",
    "\n",
    "wi = WordIndex(\n",
    "    [\n",
    "        Word(\n",
    "            word=w.word,\n",
    "            definition=w.definition,\n",
    "            pos=w.pos,\n",
    "            topic=w.topic,\n",
    "            example=clean_example(w.word, w.example),\n",
    "            syllables=h_en.syllables(w.word),\n",
    "            probably_exists=False,\n",
    "            dataset_type=wordservice_pb2.DatasetType.UD_UNFILTERED,\n",
    "            \n",
    "        ) for w in words\n",
    "        \n",
    "    ]\n",
    ")\n",
    "wi.dump_encrypted(\"../website/data/words_ud_unfiltered.enc.gz\", fernet_key=os.environ.get(\"FERNET_ENCRYPTION_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 116kB [00:00, 9.88MB/s]                    \u001b[A\u001b[A\u001b[A\n",
      "2020-05-16 20:56:47 INFO: Downloading default packages for language: en (English)...\n",
      "2020-05-16 20:56:48 INFO: File exists: /home/tdimson/stanza_resources/en/default.zip.\n",
      "2020-05-16 20:56:51 INFO: Finished downloading models and saved to /home/tdimson/stanza_resources.\n",
      "2020-05-16 20:56:51 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
      "2020-05-16 20:56:51 INFO: Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ewt     |\n",
      "| pos       | ewt     |\n",
      "=======================\n",
      "\n",
      "2020-05-16 20:56:51 INFO: Use device: gpu\n",
      "2020-05-16 20:56:51 INFO: Loading: tokenize\n",
      "2020-05-16 20:56:51 INFO: Loading: pos\n",
      "2020-05-16 20:56:52 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "wg = WordGenerator(\n",
    "    device=\"cuda:0\",\n",
    "    forward_model_path=\"/mnt/evo/projects/title-maker-pro/models/urban_dictionary_250_cleaned_lr_00005_b9_seed4/checkpoint-140000\",\n",
    "    inverse_model_path=None,\n",
    "    blacklist_path=\"/mnt/evo/projects/title-maker-pro/models/blacklist.pickle\",\n",
    "    quantize=False,\n",
    "    is_urban=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "460b19e3d1284415b1a1545225cf406f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GeneratedWord(word='cummy', pos=None, topic=None, definition='n: very big; enormous', example='That lady had a cummy penis!!', decoded='<|bod|> cummy <|bd|> n: very big; enormous <|be|> That lady had a cummy penis!! <|eod|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|>', decoded_tokens=[50257, 66, 13513, 50260, 77, 25, 845, 1263, 26, 9812, 50261, 2504, 10846, 550, 257, 10973, 1820, 16360, 3228, 50258, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wg.generate_definition(\"cummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from word_service.word_service_proto import wordservice_pb# model = AutoModelWithLMHead.from_pretrained(\"/mnt/evo/projects/title-maker-pro/models/urban_dictionary_250_cleaned_lr_00005_b9_seed4/checkpoint-140000\").to(\"cuda:0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "company_makeup",
   "language": "python",
   "name": "company_makeup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
