{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import itertools \n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../title_maker_pro\")\n",
    "from title_maker_pro import datasets\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/mnt/evo/projects/title-maker-pro/data/urban_dictionary_words.pickle\"\n",
    "with open(dataset_path, 'rb') as f:\n",
    "    dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blacklist = datasets.Blacklist.load(\"/mnt/evo/projects/title-maker-pro/models/blacklist.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_dataset = pd.DataFrame(\n",
    "    (\n",
    "        (d.word, d.meaning, d.examples[0], d.upvotes, d.downvotes, d.creation_epoch) \n",
    "        for d in itertools.chain.from_iterable(e.definitions for e in dataset.values())\n",
    "    ),\n",
    "    columns=[\"word\", \"meaning\", \"example\", \"upvotes\", \"downvotes\", \"creation_epoch\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(original, f, name):\n",
    "    n = original[f]\n",
    "    print(f\"{name} cut by {100 * len(n) / (len(original)):.2f}% ({len(original)} -> {len(n)})\")\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd_dataset.copy()\n",
    "t = cut(t, ~(pd_dataset[\"word\"].apply(blacklist.contains)), name=\"blacklist\")\n",
    "t = cut(t, ((pd_dataset[\"example\"].str.len() + pd_dataset[\"meaning\"].str.len() + pd_dataset[\"word\"].str.len()) < 250), name=\"length\")\n",
    "t = cut(t, (pd_dataset[\"upvotes\"] >= 4), name=\"upvotes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataset = OrderedDict()\n",
    "i = 0\n",
    "num_defns = 0\n",
    "for k, ud_word in dataset.items():\n",
    "    good_defns = []\n",
    "    for d in ud_word.definitions:\n",
    "        if i in valid_indexes:\n",
    "            good_defns.append(d)\n",
    "            num_defns += 1\n",
    "        i += 1\n",
    "    \n",
    "    if good_defns:\n",
    "        new = copy.deepcopy(ud_word)\n",
    "        new.definitions = good_defns\n",
    "        cleaned_dataset[k] = new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataset_path = \"/mnt/evo/projects/title-maker-pro/data/urban_dictionary_250_cleaned.pickle\"\n",
    "with open(cleaned_dataset_path, \"wb\") as f:\n",
    "    pickle.dump(cleaned_dataset, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos', use)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens(datasets.SpecialTokens.special_tokens_dict())\n",
    "blacklist = datasets.Blacklist.load(\"/mnt/evo/projects/title-maker-pro/models/blacklist_urban_dictionary.pickle\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"/mnt/evo/projects/title-maker-pro/models/urban_dictionary_250_cleaned_lr_00005_b9_seed4/checkpoint-140000\").to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2727cb2f87b84946a498bf5075aa84ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words, stats = datasets.UrbanDictionaryDataset.generate_words(\n",
    "    tokenizer, model,\n",
    "    num=10000,\n",
    "    max_iterations=500, \n",
    "    blacklist=blacklist, \n",
    "    generation_args=dict(\n",
    "        top_k=200,\n",
    "        num_return_sequences=250,\n",
    "        max_length=250,\n",
    "        do_sample=True,\n",
    "    ),\n",
    "    dedupe_titles=True,\n",
    "    # filter_proper_nouns=True,\n",
    "    min_definition_words=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter 'chnk' removed 0.08%\n",
      "Filter 'cnt' removed 1.31%\n",
      "Filter 'fg' removed 0.80%\n",
      "Filter 'fggot' removed 0.75%\n",
      "Filter 'ghetto' removed 0.57%\n",
      "Filter 'indian' removed 0.18%\n",
      "Filter 'mex' removed 0.15%\n",
      "Filter 'ngga' removed 1.25%\n",
      "Filter 'nig' removed 0.42%\n",
      "Filter 'pki' removed 0.00%\n",
      "Filter 'rape' removed 0.11%\n",
      "Filter 'sknk' removed 0.32%\n",
      "Filter 'slap' removed 0.21%\n",
      "Filter 'ultra_bad_def' removed 0.24%\n",
      "Filter 'ultra_bad_example' removed 0.03%\n",
      "Filter 'ultra_bad_word' removed 0.14%\n",
      "Total removed 6.58%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from title_maker_pro.bad_words import ULTRA_BAD_REGEX\n",
    "from website.words import WordIndex, Word\n",
    "from word_service.word_service_proto import wordservice_pb2\n",
    "\n",
    "def clean_example(w, example):\n",
    "    return re.sub(re.escape(w), w, example, flags=re.IGNORECASE)\n",
    "\n",
    "def word_filter(words):\n",
    "    filters = defaultdict(int)\n",
    "    ret = []\n",
    "    def run_over_all_text(pat, word):\n",
    "        return (\n",
    "            re.search(pat, word.word.strip(), flags=re.IGNORECASE) \n",
    "            or re.search(pat, word.definition.strip(), flags=re.IGNORECASE) \n",
    "            or re.search(pat, word.example.strip(), flags=re.IGNORECASE)\n",
    "        )\n",
    "    \n",
    "    for word in words:\n",
    "        if re.search(r\"(^|\\b)nig+\", word.word.strip()):\n",
    "            filters[\"nig\"] += 1\n",
    "        elif re.search(r\"(^|\\b)mex+\", word.word.strip()):\n",
    "            filters[\"mex\"] += 1\n",
    "        elif run_over_all_text(r\"(\\b|^)fagg+ots*\", word):\n",
    "            filters[\"fggot\"] += 1\n",
    "        elif run_over_all_text(r\"(\\b|^)f+a+g+\", word):\n",
    "            filters[\"fg\"] += 1\n",
    "        elif run_over_all_text(r\"ghettos?\", word):\n",
    "            filters[\"ghetto\"] += 1\n",
    "        elif run_over_all_text(r\"skanks*\", word):\n",
    "            filters[\"sknk\"] += 1\n",
    "        elif run_over_all_text(r\"(^|\\b)p+a+k+i+(\\b|$)\", word):\n",
    "            filters[\"pki\"]\n",
    "        elif run_over_all_text(r\"(^|\\b)cunt+\", word):\n",
    "            filters[\"cnt\"] += 1\n",
    "        elif run_over_all_text(r\"(^|\\b)indian($|\\b)\", word):\n",
    "            filters['indian'] += 1\n",
    "        elif run_over_all_text(r\"c+h+i+n+k+\", word):\n",
    "            filters['chnk'] += 1\n",
    "        elif run_over_all_text(r\"nigga+s*\", word):\n",
    "            filters['ngga'] += 1\n",
    "        elif run_over_all_text(r\"(^|\\b)slap+s*(^|\\b)\", word):\n",
    "            filters['slap'] += 1\n",
    "        elif run_over_all_text(r\"(^|\\b)r+a+p+e+s*(^|\\b)\", word):\n",
    "            filters['rape'] += 1\n",
    "        elif ULTRA_BAD_REGEX.search(word.word.strip()):\n",
    "            filters[\"ultra_bad_word\"] += 1\n",
    "        elif ULTRA_BAD_REGEX.search(word.definition.strip()):\n",
    "            filters[\"ultra_bad_def\"] += 1\n",
    "        elif ULTRA_BAD_REGEX.search(word.example.strip()):\n",
    "            filters[\"ultra_bad_example\"] += 1\n",
    "        else:\n",
    "            ret.append(word)\n",
    "            \n",
    "    for k,v in sorted(filters.items()):\n",
    "        print(f\"Filter '{k}' removed {100 * v / len(words):.2f}%\")\n",
    "        \n",
    "    print(f\"Total removed {100 * (1 - len(ret) / len(words)):.2f}%\")\n",
    "        \n",
    "    return ret\n",
    "        \n",
    "\n",
    "from hyphen import Hyphenator\n",
    "h_en = Hyphenator('en_US')\n",
    "\n",
    "wi = WordIndex(\n",
    "    [\n",
    "        Word(\n",
    "            word=w.word,\n",
    "            definition=w.definition,\n",
    "            pos=w.pos,\n",
    "            topic=w.topic,\n",
    "            example=clean_example(w.word, w.example),\n",
    "            syllables=h_en.syllables(w.word),\n",
    "            probably_exists=False,\n",
    "            dataset_type=wordservice_pb2.DatasetType.UD_UNFILTERED,\n",
    "            \n",
    "        ) for w in word_filter(words)\n",
    "        \n",
    "    ]\n",
    ")\n",
    "wi.dump_encrypted(\"../website/data/words_ud_filtered.enc.gz\", fernet_key=os.environ.get(\"FERNET_ENCRYPTION_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from word_service.word_service_proto import wordservice_pb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "company_makeup",
   "language": "python",
   "name": "company_makeup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
